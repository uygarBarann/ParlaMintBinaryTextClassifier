{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10376303,"sourceType":"datasetVersion","datasetId":6427431},{"sourceId":10377785,"sourceType":"datasetVersion","datasetId":6428475}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport time\nfrom transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom huggingface_hub import login\nfrom tqdm import tqdm\nfrom datasets import Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:35:47.717191Z","iopub.execute_input":"2025-01-05T14:35:47.717518Z","iopub.status.idle":"2025-01-05T14:36:01.708779Z","shell.execute_reply.started":"2025-01-05T14:35:47.717467Z","shell.execute_reply":"2025-01-05T14:36:01.708092Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Authenticate with Hugging Face Hub\nlogin(token=\"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\")  # Replace with your Hugging Face token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:36:20.867177Z","iopub.execute_input":"2025-01-05T14:36:20.867820Z","iopub.status.idle":"2025-01-05T14:36:21.000349Z","shell.execute_reply.started":"2025-01-05T14:36:20.867787Z","shell.execute_reply":"2025-01-05T14:36:20.999673Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load the dataset\nfile_path = \"/kaggle/input/power-tr-train-2/power-tr-train.tsv\"  \ndata = pd.read_csv(file_path, sep=\"\\t\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:36:22.075700Z","iopub.execute_input":"2025-01-05T14:36:22.076101Z","iopub.status.idle":"2025-01-05T14:36:25.282283Z","shell.execute_reply.started":"2025-01-05T14:36:22.076066Z","shell.execute_reply":"2025-01-05T14:36:25.281559Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Stratified sampling to select 10% of the data based on the label distribution\ndata_subset, _ = train_test_split(data, test_size=0.9, stratify=data[\"label\"], random_state=42)\n\n# Now data_subset is a 10% stratified sample of the original data\ntrue_labels = data_subset[\"label\"].tolist()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:36:29.080216Z","iopub.execute_input":"2025-01-05T14:36:29.080765Z","iopub.status.idle":"2025-01-05T14:36:29.114357Z","shell.execute_reply.started":"2025-01-05T14:36:29.080711Z","shell.execute_reply":"2025-01-05T14:36:29.113556Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data_subset[\"modified_text\"] = data_subset[\"text_en\"].apply(lambda x: f\"Classify whether the speaker's party is governing (label 0) or in opposition (label 1): {x}\")\nmodified_text_list = data_subset[\"modified_text\"].tolist() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:36:46.237205Z","iopub.execute_input":"2025-01-05T14:36:46.237544Z","iopub.status.idle":"2025-01-05T14:36:46.249972Z","shell.execute_reply.started":"2025-01-05T14:36:46.237514Z","shell.execute_reply":"2025-01-05T14:36:46.249065Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the Llama model and tokenizer for sequence classification\nmodel_name = \"meta-llama/Llama-3.2-1B\"  # Replace with the actual model path or name if locally hosted\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, use_auth_token=True, num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:36:48.656404Z","iopub.execute_input":"2025-01-05T14:36:48.656785Z","iopub.status.idle":"2025-01-05T14:37:51.218060Z","shell.execute_reply.started":"2025-01-05T14:36:48.656743Z","shell.execute_reply":"2025-01-05T14:37:51.217306Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e0e46475b6a4f08baae224f7e4c37fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b188ef3a6b2848e28ac1f14d7700f04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cc1153d3354aa996162565002e39fd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac460c6f9534bf6bade8c71a100361a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ca299ce362448ca9f4fc0b4c52a9ff7"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Initialize the pipeline for binary classification\nclassifier = pipeline(\n    \"text-classification\", model=model, tokenizer=tokenizer, device=0  # Set device to GPU\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:37:51.219274Z","iopub.execute_input":"2025-01-05T14:37:51.219521Z","iopub.status.idle":"2025-01-05T14:37:53.032144Z","shell.execute_reply.started":"2025-01-05T14:37:51.219476Z","shell.execute_reply":"2025-01-05T14:37:53.031146Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Perform batch inference with progress bar and estimated time\npredictions = []\nstart_time = time.time()\nlast_print_time = start_time\nfor i in tqdm(range(0, len(modified_text_list), 8), desc=\"Running inference\"):\n    batch = modified_text_list[i:i + 8]\n    batch_predictions = classifier(batch)\n    predictions.extend(batch_predictions)\n    elapsed_time = time.time() - start_time\n    processed_samples = i + len(batch)\n    estimated_total_time = (elapsed_time / processed_samples) * len(modified_text_list)\n    remaining_time = estimated_total_time - elapsed_time\n\n    # Print estimated time every 1 minute\n    current_time = time.time()\n    if current_time - last_print_time >= 60:\n        print(f\"\\rEstimated time remaining: {remaining_time / 60:.2f} minutes\")\n        last_print_time = current_time\n\nprint() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:37:53.033717Z","iopub.execute_input":"2025-01-05T14:37:53.034039Z","iopub.status.idle":"2025-01-05T14:44:42.900807Z","shell.execute_reply.started":"2025-01-05T14:37:53.034004Z","shell.execute_reply":"2025-01-05T14:44:42.899922Z"}},"outputs":[{"name":"stderr","text":"Running inference:   5%|▍         | 10/218 [00:16<05:24,  1.56s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nRunning inference:  15%|█▌        | 33/218 [01:01<06:42,  2.17s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 5.70 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  30%|██▉       | 65/218 [02:02<05:01,  1.97s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 4.80 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  45%|████▍     | 98/218 [03:04<03:24,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 3.74 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  60%|█████▉    | 130/218 [04:05<02:13,  1.52s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 2.75 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  75%|███████▌  | 164/218 [05:07<01:37,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 1.66 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  89%|████████▉ | 195/218 [06:08<00:47,  2.07s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 0.70 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference: 100%|██████████| 218/218 [06:48<00:00,  1.87s/it]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Extract predictions\npredicted_labels = [int(pred[\"label\"].split(\"_\")[-1]) for pred in predictions]  # Extract numeric label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:44:42.901781Z","iopub.execute_input":"2025-01-05T14:44:42.902040Z","iopub.status.idle":"2025-01-05T14:44:42.906470Z","shell.execute_reply.started":"2025-01-05T14:44:42.902017Z","shell.execute_reply":"2025-01-05T14:44:42.905696Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Calculate metrics\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=\"binary\")\n\n# Print metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:44:42.907748Z","iopub.execute_input":"2025-01-05T14:44:42.907965Z","iopub.status.idle":"2025-01-05T14:44:42.931115Z","shell.execute_reply.started":"2025-01-05T14:44:42.907945Z","shell.execute_reply":"2025-01-05T14:44:42.930419Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.5109\nPrecision: 0.5127\nRecall: 0.9709\nF1 Score: 0.6711\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Save the metrics and predictions to a file\nresults = pd.DataFrame({\"text\": texts, \"true_label\": true_labels, \"predicted_label\": predicted_labels})\nresults.to_csv(\"inference_results_with_metrics.csv\", index=False)\n\nprint(\"Inference completed. Results saved to 'inference_results_with_metrics.csv'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}