{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10376303,"sourceType":"datasetVersion","datasetId":6427431},{"sourceId":10377785,"sourceType":"datasetVersion","datasetId":6428475}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport time\nfrom transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom huggingface_hub import login\nfrom tqdm import tqdm\nfrom datasets import Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:49:36.374130Z","iopub.execute_input":"2025-01-05T14:49:36.374470Z","iopub.status.idle":"2025-01-05T14:49:42.798195Z","shell.execute_reply.started":"2025-01-05T14:49:36.374432Z","shell.execute_reply":"2025-01-05T14:49:42.797523Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Authenticate with Hugging Face Hub\nlogin(token=\"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\")  # Replace with your Hugging Face token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:49:42.804576Z","iopub.execute_input":"2025-01-05T14:49:42.804782Z","iopub.status.idle":"2025-01-05T14:49:42.976842Z","shell.execute_reply.started":"2025-01-05T14:49:42.804762Z","shell.execute_reply":"2025-01-05T14:49:42.976084Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load the dataset\nfile_path = \"/kaggle/input/power-tr-train-2/power-tr-train.tsv\"  \ndata = pd.read_csv(file_path, sep=\"\\t\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:49:42.977481Z","iopub.execute_input":"2025-01-05T14:49:42.977744Z","iopub.status.idle":"2025-01-05T14:49:44.831565Z","shell.execute_reply.started":"2025-01-05T14:49:42.977722Z","shell.execute_reply":"2025-01-05T14:49:44.830821Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Stratified sampling to select 20% of the data based on the label distribution\ndata_subset, _ = train_test_split(data, test_size=0.8, stratify=data[\"label\"], random_state=42)\n\n# Now data_subset is a 20% stratified sample of the original data\ntrue_labels = data_subset[\"label\"].tolist()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:49:44.844054Z","iopub.execute_input":"2025-01-05T14:49:44.844247Z","iopub.status.idle":"2025-01-05T14:49:44.862478Z","shell.execute_reply.started":"2025-01-05T14:49:44.844229Z","shell.execute_reply":"2025-01-05T14:49:44.861575Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data_subset[\"modified_text\"] = data_subset[\"text\"].apply(lambda x: f\"Konuşmacının partisinin iktidarda(etiket 0) olup olmadığını ya da muhalefette(etiket 1) mi olduğunu sınıflandırın: {x}\")\nmodified_text_list = data_subset[\"modified_text\"].tolist() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:49:53.012454Z","iopub.execute_input":"2025-01-05T14:49:53.012818Z","iopub.status.idle":"2025-01-05T14:49:53.039232Z","shell.execute_reply.started":"2025-01-05T14:49:53.012791Z","shell.execute_reply":"2025-01-05T14:49:53.038271Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the Llama model and tokenizer for sequence classification\nmodel_name = \"meta-llama/Llama-3.2-1B\"  # Replace with the actual model path or name if locally hosted\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, use_auth_token=True, num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:49:54.879637Z","iopub.execute_input":"2025-01-05T14:49:54.879929Z","iopub.status.idle":"2025-01-05T14:49:57.849980Z","shell.execute_reply.started":"2025-01-05T14:49:54.879908Z","shell.execute_reply":"2025-01-05T14:49:57.849279Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nSome weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Initialize the pipeline for binary classification\nclassifier = pipeline(\n    \"text-classification\", model=model, tokenizer=tokenizer, device=0  # Set device to GPU\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:49:59.843550Z","iopub.execute_input":"2025-01-05T14:49:59.843957Z","iopub.status.idle":"2025-01-05T14:50:01.495748Z","shell.execute_reply.started":"2025-01-05T14:49:59.843927Z","shell.execute_reply":"2025-01-05T14:50:01.494807Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Perform batch inference with progress bar and estimated time\npredictions = []\nstart_time = time.time()\nlast_print_time = start_time\nfor i in tqdm(range(0, len(modified_text_list), 8), desc=\"Running inference\"):\n    batch = modified_text_list[i:i + 8]\n    batch_predictions = classifier(batch)\n    predictions.extend(batch_predictions)\n    elapsed_time = time.time() - start_time\n    processed_samples = i + len(batch)\n    estimated_total_time = (elapsed_time / processed_samples) * len(modified_text_list)\n    remaining_time = estimated_total_time - elapsed_time\n\n    # Print estimated time every 1 minute\n    current_time = time.time()\n    if current_time - last_print_time >= 60:\n        print(f\"\\rEstimated time remaining: {remaining_time / 60:.2f} minutes\")\n        last_print_time = current_time\n\nprint() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T14:50:04.240178Z","iopub.execute_input":"2025-01-05T14:50:04.240475Z","iopub.status.idle":"2025-01-05T15:07:35.155153Z","shell.execute_reply.started":"2025-01-05T14:50:04.240447Z","shell.execute_reply":"2025-01-05T15:07:35.154237Z"}},"outputs":[{"name":"stderr","text":"Running inference:   2%|▏         | 10/435 [00:25<14:57,  2.11s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nRunning inference:   6%|▌         | 26/435 [01:01<14:30,  2.13s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 16.08 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  12%|█▏        | 51/435 [02:04<18:40,  2.92s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 15.58 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  18%|█▊        | 77/435 [03:06<15:28,  2.59s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 14.45 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  24%|██▍       | 104/435 [04:07<13:33,  2.46s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 13.11 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  30%|███       | 131/435 [05:08<14:27,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 11.90 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  35%|███▌      | 154/435 [06:08<12:22,  2.64s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 11.18 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  41%|████      | 179/435 [07:09<12:10,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 10.21 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  47%|████▋     | 204/435 [08:10<09:28,  2.46s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 9.24 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  53%|█████▎    | 230/435 [09:11<07:32,  2.21s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 8.17 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  58%|█████▊    | 252/435 [10:12<09:10,  3.01s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 7.40 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  64%|██████▍   | 278/435 [11:14<06:05,  2.33s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 6.32 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  70%|██████▉   | 303/435 [12:14<04:44,  2.16s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 5.31 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  75%|███████▌  | 327/435 [13:14<05:14,  2.91s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 4.35 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  81%|████████▏ | 354/435 [14:15<03:42,  2.74s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 3.24 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  87%|████████▋ | 378/435 [15:17<02:24,  2.54s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 2.28 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  93%|█████████▎| 403/435 [16:17<01:23,  2.61s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 1.27 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference:  99%|█████████▉| 430/435 [17:18<00:09,  1.98s/it]","output_type":"stream"},{"name":"stdout","text":"Estimated time remaining: 0.18 minutes\n","output_type":"stream"},{"name":"stderr","text":"Running inference: 100%|██████████| 435/435 [17:30<00:00,  2.42s/it]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Extract predictions\npredicted_labels = [int(pred[\"label\"].split(\"_\")[-1]) for pred in predictions]  # Extract numeric label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:13:27.558125Z","iopub.execute_input":"2025-01-05T15:13:27.558454Z","iopub.status.idle":"2025-01-05T15:13:27.563993Z","shell.execute_reply.started":"2025-01-05T15:13:27.558432Z","shell.execute_reply":"2025-01-05T15:13:27.563013Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Calculate metrics\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=\"binary\")\n\n# Print metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T15:13:27.993396Z","iopub.execute_input":"2025-01-05T15:13:27.993724Z","iopub.status.idle":"2025-01-05T15:13:28.012235Z","shell.execute_reply.started":"2025-01-05T15:13:27.993701Z","shell.execute_reply":"2025-01-05T15:13:28.011570Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.5207\nPrecision: 0.5181\nRecall: 0.9619\nF1 Score: 0.6735\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Save the metrics and predictions to a file\nresults = pd.DataFrame({\"text\": texts, \"true_label\": true_labels, \"predicted_label\": predicted_labels})\nresults.to_csv(\"inference_results_with_metrics.csv\", index=False)\n\nprint(\"Inference completed. Results saved to 'inference_results_with_metrics.csv'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}